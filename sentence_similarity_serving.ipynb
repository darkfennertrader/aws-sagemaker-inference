{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2d1c8515",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install --quiet --upgrade pip\n",
    "!pip install --quiet -U sagemaker\n",
    "!pip install -U --quiet transformers\n",
    "import torch\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6a8646ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.6.13\n",
      "torch version: 1.7.1\n",
      "transformers version: 4.11.3\n"
     ]
    }
   ],
   "source": [
    "!python -V\n",
    "print(f\"torch version: {torch.__version__}\")\n",
    "print(f\"transformers version: {transformers.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1d15e505",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1648cb4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://ai-inference-env/sentence-similarity/model.tar.gz'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# S3 directory where the model is stored\n",
    "bucket = \"ai-inference-env\"\n",
    "prefix = \"sentence-similarity\"\n",
    "key = os.path.join(prefix, \"model.tar.gz\")\n",
    "pretrained_model_data = \"s3://{}/{}\".format(bucket, key)\n",
    "pretrained_model_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "367bcb6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mre\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msys\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mlogging\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m AutoTokenizer, AutoModel\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msklearn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mmetrics\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mpairwise\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m cosine_similarity\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtest_cuda\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m test_cuda\n",
      "\n",
      "logger = logging.getLogger(\u001b[31m__name__\u001b[39;49;00m)\n",
      "logger.setLevel(logging.DEBUG)\n",
      "logger.addHandler(logging.StreamHandler(sys.stdout))\n",
      "\n",
      "\n",
      "JSON_CONTENT_TYPE = \u001b[33m\"\u001b[39;49;00m\u001b[33mapplication/json\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m# checking that cuda is available inside container\u001b[39;49;00m\n",
      "test_cuda()\n",
      "\n",
      "MODEL_PATH = \u001b[33m\"\u001b[39;49;00m\u001b[33m/opt/ml/model/\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "\n",
      "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mget_device\u001b[39;49;00m():\n",
      "    device = \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda:0\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m torch.cuda.is_available() \u001b[34melse\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "    \u001b[34mreturn\u001b[39;49;00m device\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mtext_cleaning\u001b[39;49;00m(text_to_clean):\n",
      "    text_to_clean = re.sub(\u001b[33mr\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m[.?!]+\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33m \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, text_to_clean)\n",
      "    text_to_clean = \u001b[33m\"\u001b[39;49;00m\u001b[33m \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.join(text_to_clean.split())\n",
      "    \u001b[34mreturn\u001b[39;49;00m text_to_clean.lower().strip()\n",
      "\n",
      "\n",
      "\u001b[37m# Mean Pooling - Take attention mask into account for correct averaging\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mmean_pooling\u001b[39;49;00m(model_output, attention_mask):\n",
      "    \u001b[37m# extracting the last_hidden_state tensor\u001b[39;49;00m\n",
      "    token_embeddings = model_output[\n",
      "        \u001b[34m0\u001b[39;49;00m\n",
      "    ]  \u001b[37m# First element of model_output contains all token embeddings\u001b[39;49;00m\n",
      "    input_mask_expanded = (\n",
      "        attention_mask.unsqueeze(-\u001b[34m1\u001b[39;49;00m).expand(token_embeddings.size()).float()\n",
      "    )\n",
      "    \u001b[34mreturn\u001b[39;49;00m torch.sum(token_embeddings * input_mask_expanded, \u001b[34m1\u001b[39;49;00m) / torch.clamp(\n",
      "        input_mask_expanded.sum(\u001b[34m1\u001b[39;49;00m), \u001b[36mmin\u001b[39;49;00m=\u001b[34m1e-9\u001b[39;49;00m\n",
      "    )\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mmodel_fn\u001b[39;49;00m(model_dir):\n",
      "    device = get_device()\n",
      "    model = AutoModel.from_pretrained(MODEL_PATH).eval().to(device)\n",
      "    \u001b[34mreturn\u001b[39;49;00m model\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32minput_fn\u001b[39;49;00m(serialized_input_data, content_type=JSON_CONTENT_TYPE):\n",
      "    \u001b[37m# print()\u001b[39;49;00m\n",
      "    \u001b[37m# print(\"inside input_fn\")\u001b[39;49;00m\n",
      "    \u001b[34mif\u001b[39;49;00m content_type == JSON_CONTENT_TYPE:\n",
      "        input_data = json.loads(serialized_input_data)\n",
      "        device = get_device()\n",
      "        user_input_cleaned = text_cleaning(input_data[\u001b[33m\"\u001b[39;49;00m\u001b[33muser_input\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "        true_sentence_cleaned = text_cleaning(input_data[\u001b[33m\"\u001b[39;49;00m\u001b[33mtrue_sentence\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "\n",
      "        encoded_user_input = tokenizer.encode_plus(\n",
      "            user_input_cleaned,\n",
      "            truncation=\u001b[34mTrue\u001b[39;49;00m,\n",
      "            padding=\u001b[34mTrue\u001b[39;49;00m,\n",
      "            return_tensors=\u001b[33m\"\u001b[39;49;00m\u001b[33mpt\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        ).to(device)\n",
      "\n",
      "        encoded_true_sentence = tokenizer.encode_plus(\n",
      "            true_sentence_cleaned,\n",
      "            truncation=\u001b[34mTrue\u001b[39;49;00m,\n",
      "            padding=\u001b[34mTrue\u001b[39;49;00m,\n",
      "            return_tensors=\u001b[33m\"\u001b[39;49;00m\u001b[33mpt\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        ).to(device)\n",
      "\n",
      "        \u001b[37m# we make a little bit of text cleaning\u001b[39;49;00m\n",
      "\n",
      "        \u001b[34mreturn\u001b[39;49;00m encoded_user_input, encoded_true_sentence\n",
      "\n",
      "    \u001b[34melse\u001b[39;49;00m:\n",
      "        \u001b[34mraise\u001b[39;49;00m \u001b[36mException\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mRequested unsupported ContentType in Accept: \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m + content_type)\n",
      "        \u001b[34mreturn\u001b[39;49;00m\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mpredict_fn\u001b[39;49;00m(input_data, model):\n",
      "    \u001b[37m# print()\u001b[39;49;00m\n",
      "    \u001b[37m# print(\"inside predict_fn\")\u001b[39;49;00m\n",
      "    \u001b[37m# print(f\"Got input Data: {input_data}\")\u001b[39;49;00m\n",
      "\n",
      "    encoded_user_input, encoded_true_sentence = input_data\n",
      "\n",
      "    \u001b[34mwith\u001b[39;49;00m torch.no_grad():\n",
      "        user_embedding = model(**encoded_user_input)\n",
      "        sentence_embedding = model(**encoded_true_sentence)\n",
      "\n",
      "    pooled_user_embedding = mean_pooling(\n",
      "        user_embedding, encoded_user_input[\u001b[33m\"\u001b[39;49;00m\u001b[33mattention_mask\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]\n",
      "    )\n",
      "    pooled_sentence_embedding = mean_pooling(\n",
      "        sentence_embedding, encoded_true_sentence[\u001b[33m\"\u001b[39;49;00m\u001b[33mattention_mask\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]\n",
      "    )\n",
      "\n",
      "    user_mean_pooled = pooled_user_embedding.cpu().detach().numpy()\n",
      "    sentence_mean_pooled = pooled_sentence_embedding.cpu().detach().numpy()\n",
      "\n",
      "    similarity = cosine_similarity(user_mean_pooled, sentence_mean_pooled)\n",
      "\n",
      "    \u001b[34mreturn\u001b[39;49;00m similarity\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32moutput_fn\u001b[39;49;00m(prediction_output, accept=JSON_CONTENT_TYPE):\n",
      "    \u001b[37m# print()\u001b[39;49;00m\n",
      "    \u001b[37m# print(\"inside output_fn\")\u001b[39;49;00m\n",
      "    \u001b[37m# print(f\"prediction output: {prediction_output}\")\u001b[39;49;00m\n",
      "    \u001b[37m# print(type(prediction_output))\u001b[39;49;00m\n",
      "    \u001b[34mif\u001b[39;49;00m accept == JSON_CONTENT_TYPE:\n",
      "        prediction_output = {\u001b[33m\"\u001b[39;49;00m\u001b[33msimilarity\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[36mstr\u001b[39;49;00m(prediction_output.item())}\n",
      "        \u001b[34mreturn\u001b[39;49;00m json.dumps(prediction_output), accept\n",
      "\n",
      "    \u001b[34mraise\u001b[39;49;00m \u001b[36mException\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mRequested unsupported ContentType in Accept: \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m + accept)\n"
     ]
    }
   ],
   "source": [
    "!pygmentize code_sentence_similarity/inference.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c81b337b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#instance_type = \"local_gpu\" # or \"local\"\n",
    "instance_type = \"ml.g4dn.xlarge\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "53d2a1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "role = sagemaker.get_execution_role()\n",
    "sess = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "60a08db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch.model import PyTorchModel\n",
    "\n",
    "pytorch_model = PyTorchModel(\n",
    "    model_data=pretrained_model_data,\n",
    "    role=role,\n",
    "    framework_version=\"1.8.1\",\n",
    "    source_dir=\"./code_sentence_similarity\",\n",
    "    py_version=\"py3\",\n",
    "    entry_point=\"inference.py\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "de327e8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------!"
     ]
    }
   ],
   "source": [
    "predictor = pytorch_model.deploy(endpoint_name= \"sentence-similarity\", initial_instance_count=1, instance_type=instance_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "15831909",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.serializer = sagemaker.serializers.JSONSerializer()\n",
    "predictor.deserializer = sagemaker.deserializers.JSONDeserializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cc77956b",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_sentence = \"I love going to the beach\"\n",
    "ground_truth = \"the beach is beautiful\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f18ba310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'similarity': '0.6418977975845337'}\n",
      "result: 0.6418977975845337\n"
     ]
    }
   ],
   "source": [
    "result = predictor.predict({\"user_input\": user_sentence, \"true_sentence\": ground_truth})\n",
    "print(result)\n",
    "print(f\"result: {result['similarity']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b069aa9",
   "metadata": {},
   "source": [
    "### Computes inference average time ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "49f930e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average inference on GPU is: 0.0347 ms\n"
     ]
    }
   ],
   "source": [
    "inference_time = []\n",
    "for _ in range(50):\n",
    "    start = time.time()\n",
    "    predictor.predict({\"user_input\": user_sentence, \"true_sentence\": ground_truth})\n",
    "    inference_time.append(time.time()-start)\n",
    "    \n",
    "print(f\"Average inference on GPU is: {sum(inference_time)/len(inference_time):.3} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbda79a0",
   "metadata": {},
   "source": [
    "### Clean Up ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dd3abfe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The endpoint attribute has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    }
   ],
   "source": [
    "#predictor.delete_endpoint(predictor.endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34afadf9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_latest_p36",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
